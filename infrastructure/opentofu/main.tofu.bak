module "s3" {
  source = "./state" 

  s3 = var.s3
  bucket_name = var.bucket_name
}

module "talos_cluster" {
  source = "./talos"

  providers = {
    proxmox = proxmox
  }

  image = var.talos_image
  cluster = var.talos_cluster_config
  nodes = var.talos_nodes
}

# Control Plane Nodes
resource "proxmox_virtual_environment_vm" "control_plane" {
  count = var.control_plane_count
  
  name        = "talos-cp-${format("%02d", count.index + 1)}"
  node_name   = var.proxmox_node
  vm_id       = var.control_plane_vmid_start + count.index
  
  operating_system {
    type = "l26"
  }

  bios = "seabios"
  
  cpu {
    cores   = var.control_plane_cores
    sockets = 1
    type    = "host"
  }

  memory {
    dedicated = var.control_plane_memory
  }  

  disk {
    datastore_id = var.storage_pool
    size         = var.control_plane_disk_size
    interface    = "scsi0"
    iothread     = true
    discard      = "on"
    ssd          = true
  }

  cdrom {
    file_id = "local:iso/nocloud-amd64.iso"
    interface = "ide2"
  }

  scsi_hardware = "virtio-scsi-single"
  boot_order    = ["ide2", "scsi0"]

  network_device {
    model  = "virtio"
    bridge = var.network_bridge
  }
  
  agent {
    enabled = true
  }
  
  stop_on_destroy = true
  on_boot         = true
  started         = true
  
  lifecycle {
    ignore_changes = [
      cdrom,
    ]
  }
}

# Worker Nodes
resource "proxmox_virtual_environment_vm" "worker" {
  count = var.worker_count
  
  name      = "talos-worker-${format("%02d", count.index + 1)}"
  node_name = var.proxmox_node
  vm_id     = var.worker_vmid_start + count.index
  
  operating_system {
    type = "l26"
  } 

  bios = "seabios"

  cpu {
    cores   = var.worker_cores
    sockets = 1 
    type    = "host"
  }
  
  memory {
    dedicated = var.worker_memory
  }  

  disk {
    datastore_id = var.storage_pool
    size         = var.worker_disk_size
    interface    = "scsi0"
    iothread     = true
    discard      = "on"
    ssd          = true
  }

  cdrom {
    file_id = "local:iso/nocloud-amd64.iso"
    interface = "ide2"
  }

  scsi_hardware = "virtio-scsi-single"
  boot_order    = ["ide2", "scsi0"]

  network_device {
    model  = "virtio"
    bridge = var.network_bridge
  }

  agent {
    enabled = true
  }
  
  stop_on_destroy = true
  on_boot         = true
  started         = true

  lifecycle {
    ignore_changes = [
      cdrom,
    ]
  }
}

# Wait for VMs to boot
resource "time_sleep" "wait_for_vms" {
  depends_on = [
    proxmox_virtual_environment_vm.control_plane,
    proxmox_virtual_environment_vm.worker
  ]
  
  create_duration = "30s"
}

# Single orchestration point - let the script handle everything else
resource "null_resource" "configure_cluster" {
  provisioner "local-exec" {
    command = "${path.module}/../../scripts/configure-and-bootstrap.sh"
    
    environment = {
      TALOS_DIR = "${path.module}/../../infrastructure/talos"
      SKIP_VM_CREATION = "true"  # VMs already created by Terraform
    }
  }
  
  depends_on = [time_sleep.wait_for_vms]
}

# Output useful information
output "cluster_status" {
  value = "VMs created. Configuration and bootstrap handled by scripts."
  depends_on = [null_resource.configure_cluster]
}

output "next_steps" {
  value = <<-EOT
    If automation fails, run manually:
    1. Check VM status: ssh root@10.0.70.10 'qm list | grep talos'
    2. Apply configs: ./scripts/check-and-apply.sh
    3. Bootstrap: 
       export TALOSCONFIG=infrastructure/talos/talosconfig
       talosctl bootstrap --nodes 10.0.70.70 --endpoints 10.0.70.70
    4. Get kubeconfig: talosctl kubeconfig
    5. Check nodes: kubectl get nodes
  EOT
  depends_on = [null_resource.configure_cluster]
}
