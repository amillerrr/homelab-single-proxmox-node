apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cluster-alerts
  namespace: monitoring
spec:
  groups:
  # Cluster Health Alerts
  - name: cluster-health
    interval: 30s
    rules:
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
        component: infrastructure
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        description: "Node {{ $labels.instance }} has been down for more than 5 minutes."
        
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 10m
      labels:
        severity: warning
        component: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"
        
    - alert: HighCPUUsage
      expr: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) * 100 > 80
      for: 10m
      labels:
        severity: warning
        component: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"
        
    - alert: DiskSpaceLow
      expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
      for: 5m
      labels:
        severity: warning
        component: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk space is below 15% (current value: {{ $value }}%)"

  # Kubernetes Alerts
  - name: kubernetes
    interval: 30s
    rules:
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[5m]) > 0.5
      for: 5m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value }} times in the last 5 minutes"
        
    - alert: PodNotReady
      expr: kube_pod_status_phase{phase!="Running", phase!="Succeeded"} == 1
      for: 15m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
        description: "Pod has been in {{ $labels.phase }} state for more than 15 minutes"
        
    - alert: DeploymentReplicasMismatch
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch"
        description: "Deployment has {{ $value }} replicas available, but {{ $labels.spec_replicas }} are desired"
        
    - alert: PersistentVolumeError
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending"} == 1
      for: 5m
      labels:
        severity: critical
        component: storage
      annotations:
        summary: "PersistentVolume {{ $labels.persistentvolume }} has issues"
        description: "PersistentVolume is in {{ $labels.phase }} state"

  # Cilium Alerts
  - name: cilium
    interval: 30s
    rules:
    - alert: CiliumAgentNotReady
      expr: cilium_agent_bootstrap_seconds == 0
      for: 5m
      labels:
        severity: critical
        component: network
      annotations:
        summary: "Cilium agent not ready on {{ $labels.pod }}"
        description: "Cilium agent has not completed bootstrap for more than 5 minutes"
        
    - alert: CiliumHighDropRate
      expr: rate(cilium_drop_count_total[5m]) > 100
      for: 5m
      labels:
        severity: warning
        component: network
      annotations:
        summary: "High packet drop rate detected"
        description: "Cilium is dropping {{ $value }} packets per second (reason: {{ $labels.reason }})"
        
    - alert: CiliumEndpointNotReady
      expr: cilium_endpoint_state{state!="ready"} == 1
      for: 10m
      labels:
        severity: warning
        component: network
      annotations:
        summary: "Cilium endpoint not ready"
        description: "Endpoint {{ $labels.endpoint_id }} is in {{ $labels.state }} state"
        
    - alert: CiliumPolicyUpdateFailure
      expr: rate(cilium_policy_update_errors_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
        component: network
      annotations:
        summary: "Cilium policy update failures"
        description: "Cilium is experiencing policy update failures ({{ $value }} errors per second)"

  # Longhorn Storage Alerts
  - name: longhorn
    interval: 30s
    rules:
    - alert: LonghornVolumeError
      expr: longhorn_volume_robustness == 3
      for: 5m
      labels:
        severity: critical
        component: storage
      annotations:
        summary: "Longhorn volume {{ $labels.volume }} is in error state"
        description: "Volume has no healthy replicas available"
        
    - alert: LonghornVolumeDegraded
      expr: longhorn_volume_robustness == 2
      for: 10m
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Longhorn volume {{ $labels.volume }} is degraded"
        description: "Volume is running with reduced replicas"
        
    - alert: LonghornNodeStorageFull
      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 85
      for: 5m
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Longhorn node storage nearly full"
        description: "Node {{ $labels.node }} storage is {{ $value }}% full"

  # Application Alerts
  - name: application
    interval: 30s
    rules:
    - alert: APIHighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="api-service"}[5m])) > 1
      for: 10m
      labels:
        severity: warning
        component: application
      annotations:
        summary: "API high latency"
        description: "95th percentile latency is {{ $value }}s"
        
    - alert: APIHighErrorRate
      expr: rate(http_requests_total{job="api-service", status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
        component: application
      annotations:
        summary: "API high error rate"
        description: "API error rate is {{ $value | humanizePercentage }}"
        
    - alert: DatabaseConnectionFailure
      expr: pg_up == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "Database connection failure"
        description: "Cannot connect to PostgreSQL database"
        
    - alert: RedisDown
      expr: redis_up == 0
      for: 5m
      labels:
        severity: warning
        component: cache
      annotations:
        summary: "Redis cache is down"
        description: "Redis cache has been unavailable for 5 minutes"

  # Certificate Alerts
  - name: certificates
    interval: 1h
    rules:
    - alert: CertificateExpiringSoon
      expr: certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 3600
      for: 1h
      labels:
        severity: warning
        component: certificates
      annotations:
        summary: "Certificate expiring soon"
        description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expires in {{ $value | humanizeDuration }}"
        
    - alert: CertificateExpired
      expr: certmanager_certificate_expiration_timestamp_seconds - time() < 0
      for: 10m
      labels:
        severity: critical
        component: certificates
      annotations:
        summary: "Certificate expired"
        description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} has expired"

---
